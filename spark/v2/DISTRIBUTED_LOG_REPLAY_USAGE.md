# Distributed Log Replay ä½¿ç”¨æŒ‡å—

## å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```java
import io.delta.spark.internal.v2.read.DistributedLogReplayHelper;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

// åˆ›å»ºSparkSession
SparkSession spark = SparkSession.builder()
    .appName("Delta V2 Distributed Replay")
    .getOrCreate();

// è·å–Delta snapshot
Snapshot snapshot = ...; // ä»Table.forPath()æˆ–SnapshotBuilderè·å–

// æ‰§è¡Œåˆ†å¸ƒå¼log replay
Dataset<Row> addFiles = DistributedLogReplayHelper.distributedLogReplay(
    spark,
    snapshot,
    50  // num partitions
);

// æŸ¥çœ‹ç»“æœ
addFiles.show();
addFiles.count(); // æ–‡ä»¶æ€»æ•°
```

### 2. é›†æˆåˆ°SparkScan

åœ¨`SparkScan.java`ä¸­ä¿®æ”¹`planScanFiles()`æ–¹æ³•ï¼š

```java
// åœ¨SparkScanç±»ä¸­æ·»åŠ é…ç½®
private static final boolean USE_DISTRIBUTED_REPLAY = 
    Boolean.parseBoolean(System.getProperty(
        "spark.databricks.delta.v2.distributedLogReplay.enabled", 
        "false"));

private static final int DISTRIBUTED_REPLAY_FILE_THRESHOLD = 
    Integer.parseInt(System.getProperty(
        "spark.databricks.delta.v2.distributedLogReplay.fileThreshold",
        "10000"));

private void planScanFiles() {
    // åˆ¤æ–­æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼æ–¹æ¡ˆ
    boolean useDistributed = USE_DISTRIBUTED_REPLAY && 
        shouldUseDistributedReplay();
    
    if (useDistributed) {
        planScanFilesDistributed();
    } else {
        planScanFilesWithKernel();
    }
}

private boolean shouldUseDistributedReplay() {
    // ç®€å•ä¼°ç®—ï¼šå¦‚æœdeltaæ–‡ä»¶æ•°é‡è¶…è¿‡é˜ˆå€¼ï¼Œä½¿ç”¨åˆ†å¸ƒå¼
    if (initialSnapshot instanceof SnapshotImpl) {
        SnapshotImpl impl = (SnapshotImpl) initialSnapshot;
        LogSegment logSegment = impl.getLogSegment();
        int estimatedFiles = logSegment.getDeltas().size() * 1000; // ç²—ç•¥ä¼°ç®—
        return estimatedFiles > DISTRIBUTED_REPLAY_FILE_THRESHOLD;
    }
    return false;
}

private void planScanFilesDistributed() {
    SparkSession spark = SparkSession.active();
    
    // Step 1: åˆ†å¸ƒå¼log replay
    Dataset<Row> addFiles = DistributedLogReplayHelper.distributedLogReplay(
        spark,
        initialSnapshot,
        50  // numPartitions
    );
    
    // Step 2: è§£æstats
    Dataset<Row> withStats = addFiles.withColumn("stats_parsed",
        from_json(col("stats"), DistributedLogReplayHelper.getStatsSchema()));
    
    // Step 3: åº”ç”¨filters
    Dataset<Row> filtered = applyFiltersOnDataFrame(withStats);
    
    // Step 4: Collectç»“æœ
    List<Row> files = filtered.collectAsList();
    
    // Step 5: è½¬æ¢ä¸ºPartitionedFile
    for (Row row : files) {
        String path = row.getAs("path");
        long size = row.getAs("size");
        scala.collection.immutable.Map<String, String> partVals = row.getAs("partitionValues");
        
        // æ„å»ºPartitionedFileï¼ˆä½¿ç”¨ç°æœ‰çš„PartitionUtilsï¼‰
        // ... (ä¸ç°æœ‰é€»è¾‘ç›¸åŒ)
        
        totalBytes += size;
        partitionedFiles.add(partitionedFile);
    }
    
    planned = true;
}

private void planScanFilesWithKernel() {
    // ä¿ç•™ç°æœ‰çš„Kernelå®ç°
    final Engine tableEngine = DefaultEngine.create(hadoopConf);
    final Iterator<FilteredColumnarBatch> scanFileBatches = 
        kernelScan.getScanFiles(tableEngine);
    // ... (ç°æœ‰ä»£ç )
}

private Dataset<Row> applyFiltersOnDataFrame(Dataset<Row> df) {
    // åº”ç”¨partition filters
    for (Filter filter : pushedToKernelFilters) {
        if (isPartitionFilter(filter)) {
            df = df.filter(convertFilterToSparkExpr(filter));
        }
    }
    
    // åº”ç”¨data skipping filters
    for (Filter filter : dataFilters) {
        df = df.filter(convertToDataSkippingExpr(filter));
    }
    
    return df;
}
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. è°ƒæ•´åˆ†åŒºæ•°é‡

```properties
# æ ¹æ®è¡¨å¤§å°å’Œé›†ç¾¤å¤§å°è°ƒæ•´
# å°è¡¨ï¼ˆ< 10K filesï¼‰ï¼š10-20 partitions
spark.databricks.delta.v2.distributedLogReplay.numPartitions = 20

# ä¸­è¡¨ï¼ˆ10K-100K filesï¼‰ï¼š30-50 partitions
spark.databricks.delta.v2.distributedLogReplay.numPartitions = 50

# å¤§è¡¨ï¼ˆ> 100K filesï¼‰ï¼š50-100 partitions
spark.databricks.delta.v2.distributedLogReplay.numPartitions = 100
```

### 2. è‡ªé€‚åº”ç­–ç•¥

```java
private int getNumPartitions() {
    if (initialSnapshot instanceof SnapshotImpl) {
        SnapshotImpl impl = (SnapshotImpl) initialSnapshot;
        LogSegment logSegment = impl.getLogSegment();
        int deltaCount = logSegment.getDeltas().size();
        
        // è‡ªé€‚åº”åˆ†åŒºæ•°
        if (deltaCount < 10) return 10;
        if (deltaCount < 50) return 20;
        if (deltaCount < 100) return 50;
        return 100;
    }
    return 50; // default
}
```

### 3. ç¼“å­˜ä¸­é—´ç»“æœ

å¯¹äºé¢‘ç¹æŸ¥è¯¢çš„è¡¨ï¼Œå¯ä»¥ç¼“å­˜log replayç»“æœï¼š

```java
Dataset<Row> addFiles = DistributedLogReplayHelper.distributedLogReplay(
    spark, snapshot, numPartitions
);

// ç¼“å­˜ç»“æœï¼ˆå¦‚æœåç»­æœ‰å¤šä¸ªæŸ¥è¯¢ï¼‰
addFiles.cache();
addFiles.count(); // è§¦å‘ç¼“å­˜
```

## ç›‘æ§å’Œè°ƒè¯•

### 1. æ€§èƒ½ç›‘æ§

```java
long startTime = System.nanoTime();

Dataset<Row> addFiles = DistributedLogReplayHelper.distributedLogReplay(
    spark, snapshot, numPartitions
);

long replayTime = System.nanoTime() - startTime;
System.out.println("Log replay time: " + replayTime / 1_000_000 + " ms");

List<Row> files = addFiles.collectAsList();
long totalTime = System.nanoTime() - startTime;
System.out.println("Total time: " + totalTime / 1_000_000 + " ms");
System.out.println("Files found: " + files.size());
```

### 2. Spark UIæŸ¥çœ‹

- æ‰“å¼€Spark UI: http://localhost:4040
- æŸ¥çœ‹SQL tabï¼Œæ‰¾åˆ°distributed log replayçš„stage
- æ£€æŸ¥ï¼š
  - Taskæ•°é‡æ˜¯å¦ç­‰äºnumPartitions
  - Taskåˆ†å¸ƒæ˜¯å¦å‡åŒ€
  - Shuffle read/writeé‡

### 3. æ—¥å¿—è°ƒè¯•

```java
// å¼€å¯DEBUGæ—¥å¿—
spark.sparkContext().setLogLevel("DEBUG");

// æŸ¥çœ‹DataFrameçš„æ‰§è¡Œè®¡åˆ’
addFiles.explain(true);

// æŸ¥çœ‹å®é™…çš„æ–‡ä»¶æ•°é‡
System.out.println("Checkpoint files: " + logSegment.getCheckpoints().size());
System.out.println("Delta files: " + logSegment.getDeltas().size());
```

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆå°è¡¨åè€Œå˜æ…¢äº†ï¼Ÿ

**A**: åˆ†å¸ƒå¼å¤„ç†æœ‰task scheduling overheadã€‚å¯¹äºå°è¡¨ï¼ˆ< 1000 filesï¼‰ï¼Œä¸²è¡Œå¤„ç†æ›´å¿«ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨è‡ªé€‚åº”ç­–ç•¥ï¼Œæ ¹æ®æ–‡ä»¶æ•°é‡é€‰æ‹©å¤„ç†æ–¹å¼ã€‚

```java
if (estimatedFiles < DISTRIBUTED_REPLAY_FILE_THRESHOLD) {
    planScanFilesWithKernel(); // ä¸²è¡Œ
} else {
    planScanFilesDistributed(); // åˆ†å¸ƒå¼
}
```

### Q2: å¦‚ä½•ç¡®ä¿log replayæ­£ç¡®æ€§ï¼Ÿ

**A**: å…³é”®æ˜¯ä¿è¯add/remove reconciliationæ­£ç¡®ã€‚

```java
// å…³é”®ä»£ç 
Dataset<Row> replayed = allActions
    .groupBy("file_path_canonical")
    .agg(
        last("add", true).as("add"),       // ignoreNulls=true
        last("remove", true).as("remove")
    )
    .filter(col("add").isNotNull().and(col("remove").isNull()));
```

**æµ‹è¯•**ï¼š
1. åˆ›å»ºæµ‹è¯•è¡¨ï¼Œå¤šæ¬¡add/removeåŒä¸€ä¸ªæ–‡ä»¶
2. å¯¹æ¯”Kernelå’Œåˆ†å¸ƒå¼æ–¹æ¡ˆçš„ç»“æœ
3. ç¡®ä¿æ–‡ä»¶åˆ—è¡¨å’Œstatså®Œå…¨ä¸€è‡´

### Q3: Deletion Vectorå¦‚ä½•å¤„ç†ï¼Ÿ

**A**: Deletion Vectorä¿¡æ¯åœ¨AddFileçš„`deletionVector`å­—æ®µä¸­ã€‚

```java
// ç¡®ä¿ä¿ç•™DVå­—æ®µ
Dataset<Row> addFiles = replayed.select(
    col("add.path"),
    col("add.size"),
    col("add.stats"),
    col("add.deletionVector")  // é‡è¦ï¼
);

// è®¡ç®—logical records
withStats.withColumn("numLogicalRecords",
    when(col("deletionVector").isNull(),
        col("stats_parsed.numRecords"))
    .otherwise(
        col("stats_parsed.numRecords")
            .minus(col("deletionVector.cardinality"))
    )
);
```

### Q4: V2 Checkpointå¦‚ä½•åˆ©ç”¨ï¼Ÿ

**A**: V2 Checkpointå·²ç»æœ‰structured statsï¼Œå¯ä»¥ç›´æ¥ç”¨predicate pushdownã€‚

```java
// æ£€æµ‹V2 checkpoint
boolean isV2Checkpoint = checkpointPath.contains(".checkpoint.parquet");

if (isV2Checkpoint) {
    // V2 checkpoint - ç›´æ¥filter
    Dataset<Row> checkpointDF = spark.read()
        .format("parquet")
        .load(checkpointPath)
        .filter("add.stats_parsed.maxValues.age > 18");  // Parquet pushdown!
} else {
    // V1 checkpoint - éœ€è¦æ‰‹åŠ¨è§£æ
    Dataset<Row> checkpointDF = spark.read()
        .format("parquet")
        .load(checkpointPath)
        .withColumn("stats_parsed", from_json(col("add.stats"), statsSchema))
        .filter(col("stats_parsed.maxValues.age").gt(18));
}
```

## ä¸V1å¯¹æ¯”

| ç‰¹æ€§ | V1 (PrepareDeltaScan) | V2 (Distributed Replay) | çŠ¶æ€ |
|------|----------------------|-------------------------|------|
| åˆ†å¸ƒå¼log replay | âœ… | âœ… | âœ… å·²å®ç° |
| åˆ†å¸ƒå¼statsè§£æ | âœ… | âœ… | âœ… å·²å®ç° |
| Partition filtering | âœ… | âš ï¸ | ğŸš§ éœ€å®Œå–„ |
| Data skipping | âœ… | âš ï¸ | ğŸš§ éœ€å®Œå–„ |
| IsNull expansion | âœ… | âŒ | ğŸ“‹ å¾…å®ç° |
| StartsWith optimization | âœ… | âŒ | ğŸ“‹ å¾…å®ç° |
| Generated columns | âœ… | âŒ | ğŸ“‹ å¾…å®ç° |
| Limit pushdown | âœ… | âŒ | ğŸ“‹ å¾…å®ç° |
| Metadata-only queries | âœ… | âŒ | ğŸ“‹ å¾…å®ç° |

## ä¸‹ä¸€æ­¥

1. **å®Œå–„data skipping filters**
   - å®ç°å®Œæ•´çš„filter conversion
   - æ”¯æŒæ‰€æœ‰V1çš„ä¼˜åŒ–

2. **æ€§èƒ½æµ‹è¯•**
   - ä¸åŒè¡¨å¤§å°çš„benchmark
   - ä¸V1æ€§èƒ½å¯¹æ¯”

3. **é›†æˆæµ‹è¯•**
   - å„ç§Deltaè¡¨æ ¼å¼
   - å„ç§æŸ¥è¯¢æ¨¡å¼

4. **ç”Ÿäº§åŒ–**
   - é”™è¯¯å¤„ç†
   - ç›‘æ§æŒ‡æ ‡
   - é…ç½®ç®¡ç†
