/*
 * Copyright (2021) The Delta Lake Project Authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.delta

import org.apache.spark.sql.delta.metering.DeltaLogging
import org.apache.spark.sql.delta.sources.DeltaSourceUtils._
import org.apache.spark.sql.delta.stats.{DeltaFileStatistics, DeltaJobStatisticsTracker}
import org.apache.spark.sql.delta.util.JsonUtils
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.Path

import org.apache.spark.sql.{Column, DataFrame, Dataset, SparkSession}
import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute
import org.apache.spark.sql.catalyst.expressions.{Attribute, Expression}
import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, LogicalPlan}
import org.apache.spark.sql.execution.datasources.WriteTaskStats
import org.apache.spark.sql.functions.{array, max, min, to_json}
import org.apache.spark.sql.types.{MetadataBuilder, StructField, StructType}

/**
 * Provide utility methods related to IDENTITY column support for Delta.
 */
object IdentityColumn extends DeltaLogging {
  case class IdentityInfo(start: Long, step: Long, highWaterMark: Option[Long])
  // Default start and step configuration if not specified by user.
  val defaultStart = 1
  val defaultStep = 1
  // Operation types in usage logs.
  // When IDENTITY columns are defined.
  val opTypeDefinition = "delta.identityColumn.definition"
  // When table with IDENTITY columns are written into.
  val opTypeWrite = "delta.identityColumn.write"

  // Return all the IDENTITY columns from `schema`.
  def getIdentityColumns(schema: StructType): Seq[StructField] = {
    schema.filter(ColumnWithDefaultExprUtils.isIdentityColumn)
  }

  // Return the number of IDENTITY columns in `schema`.
  private def getNumberOfIdentityColumns(schema: StructType): Int = {
    getIdentityColumns(schema).size
  }

  // Create expression to generate IDENTITY values for the column `field`.
  def createIdentityColumnGenerationExpr(field: StructField): Expression = {
    val info = IdentityColumn.getIdentityInfo(field)
    GenerateIdentityValues(info.start, info.step, info.highWaterMark)
  }

  // Create a column to generate IDENTITY values for the column `field`.
  def createIdentityColumnGenerationExprAsColumn(field: StructField): Column = {
    new Column(createIdentityColumnGenerationExpr(field)).alias(field.name)
  }

  /**
   * Create a stats tracker to collect IDENTITY column high water marks if its values are system
   * generated.
   *
   * @param spark The SparkSession associated with this query.
   * @param hadoopConf The Hadoop configuration object to use on an executor.
   * @param path Root Reservoir path
   * @param schema The schema of the table to be written into.
   * @param statsDataSchema The schema of the output data (this does not include partition columns).
   * @param trackHighWaterMarks Column names for which we should track high water marks.
   * @return The stats tracker.
   */
  def createIdentityColumnStatsTracker(
      spark: SparkSession,
      hadoopConf: Configuration,
      path: Path,
      schema: StructType,
      statsDataSchema: Seq[Attribute],
      trackHighWaterMarks: Set[String]
    ) : Option[DeltaIdentityColumnStatsTracker] = {
    if (trackHighWaterMarks.isEmpty) return None
    val identityColumnInfo = schema
      .filter(f => trackHighWaterMarks.contains(f.name))
      .map(f => DeltaColumnMapping.getPhysicalName(f) ->  // Get identity column physical names
        (f.metadata.getLong(IDENTITY_INFO_STEP) > 0L))
    // We should have found all IDENTITY columns to track high water marks.
    assert(identityColumnInfo.size == trackHighWaterMarks.size,
      s"expect: $trackHighWaterMarks, found (physical names): ${identityColumnInfo.map(_._1)}")
    // Build the expression to collect high water marks of all IDENTITY columns as a single
    // expression. It is essentially a json array containing one max or min aggregate expression
    // for each IDENTITY column.
    //
    // Example: for the following table
    //
    //   CREATE TABLE t1 (
    //     id1 BIGINT GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),
    //     id2 BIGINT GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY -1),
    //     value STRING
    //   ) USING delta;
    //
    // The expression will be: to_json(array(max(id1), min(id2)))
    val aggregates = identityColumnInfo.map {
      case (name, positiveStep) =>
        val col = new Column(UnresolvedAttribute.quoted(name))
        if (positiveStep) max(col) else min(col)
    }
    val unresolvedExpr = to_json(array(aggregates: _*))
    // Resolve the collection expression by constructing a query to select the expression from a
    // table with the statsSchema and get the analyzed expression.
    val resolvedExpr = Dataset.ofRows(spark, LocalRelation(statsDataSchema))
      .select(unresolvedExpr).queryExecution.analyzed.expressions.head
    Some(new DeltaIdentityColumnStatsTracker(
      hadoopConf,
      path,
      statsDataSchema,
      resolvedExpr,
      identityColumnInfo
    ))
  }

  /**
   * Return a new schema with IDENTITY high water marks updated in the schema.
   * The new high watermarks are decided based on the `updatedIdentityHighWaterMarks` and old high
   * watermark values present in the passed `schema`.
   */
  def updateSchema(
      schema: StructType,
      updatedIdentityHighWaterMarks: Seq[(String, Long)]) : StructType = {
    val updatedIdentityHighWaterMarksGrouped =
      updatedIdentityHighWaterMarks.groupBy(_._1).mapValues(v => v.map(_._2))
    StructType(schema.map { f =>
      updatedIdentityHighWaterMarksGrouped.get(DeltaColumnMapping.getPhysicalName(f)) match {
        case Some(newWatermarks) if ColumnWithDefaultExprUtils.isIdentityColumn(f) =>
          val oldIdentityInfo = getIdentityInfo(f)
          val positiveStep = oldIdentityInfo.step > 0
          val newHighWaterMark = if (positiveStep) {
            oldIdentityInfo.highWaterMark.map(Math.max(_, newWatermarks.max))
              .getOrElse(newWatermarks.max)
          } else {
            oldIdentityInfo.highWaterMark.map(Math.min(_, newWatermarks.min))
              .getOrElse(newWatermarks.min)
          }
          val builder = new MetadataBuilder()
            .withMetadata(f.metadata)
            .putLong(IDENTITY_INFO_HIGHWATERMARK, newHighWaterMark)
          f.copy(metadata = builder.build())
        case _ =>
          f
      }
    })
  }
  def logTableCreation(deltaLog: DeltaLog, schema: StructType): Unit = {
    val numIdentityColumns = getNumberOfIdentityColumns(schema)
    if (numIdentityColumns != 0) {
      recordDeltaEvent(
        deltaLog,
        opTypeDefinition,
        data = Map(
          "numIdentityColumns" -> numIdentityColumns
        )
      )
    }
  }

  def logTableWrite(
      snapshot: Snapshot,
      generatedIdentityColumns: Set[String],
      numInsertedRowsOpt: Option[Long]): Unit = {
    val identityColumns = getIdentityColumns(snapshot.schema)
    if (identityColumns.nonEmpty) {
      val explicitIdentityColumns = identityColumns.filter {
        f => !generatedIdentityColumns.contains(f.name)
      }.map(_.name)
      recordDeltaEvent(
        snapshot.deltaLog,
        opTypeWrite,
        data = Map(
          "numInsertedRows" -> numInsertedRowsOpt,
          "generatedIdentityColumnNames" -> generatedIdentityColumns.mkString(","),
          "generatedIdentityColumnCount" -> generatedIdentityColumns.size,
          "explicitIdentityColumnNames" -> explicitIdentityColumns.mkString(","),
          "explicitIdentityColumnCount" -> explicitIdentityColumns.size
        )
      )
    }
  }
  // Return IDENTITY information of column `field`. Caller must ensure `isIdentityColumn(field)`
  // is true.
  def getIdentityInfo(field: StructField): IdentityInfo = {
    val md = field.metadata
    val start = md.getLong(IDENTITY_INFO_START)
    val step = md.getLong(IDENTITY_INFO_STEP)
    // If system hasn't generated IDENTITY values for this column (either it hasn't been
    // inserted into, or every inserts provided values for this IDENTITY column), high water mark
    // field will not present in column metadata. In this case, high water mark will be set to
    // (start - step) so that the first value generated is start (high water mark + step).
    val highWaterMark = if (md.contains(IDENTITY_INFO_HIGHWATERMARK)) {
        Some(md.getLong(IDENTITY_INFO_HIGHWATERMARK))
      } else {
        None
      }
    IdentityInfo(start, step, highWaterMark)
  }
}

/**
 * Stats tracker for IDENTITY column high water marks. The only difference between this class and
 * `DeltaJobStatisticsTracker` is how the stats are aggregated on the driver.
 *
 * @param hadoopConf The Hadoop configuration object to use on an executor.
 * @param path Root Reservoir path
 * @param dataCols Resolved data (i.e. non-partitionBy) columns of the dataframe to be written.
 * @param statsColExpr The expression to collect high water marks.
 * @param identityColumnInfo Information of IDENTITY columns. It contains a pair of column name
 *                           and whether it has a positive step for each IDENTITY column.
 */
class DeltaIdentityColumnStatsTracker(
    @transient private val hadoopConf: Configuration,
    @transient path: Path,
    dataCols: Seq[Attribute],
    statsColExpr: Expression,
    val identityColumnInfo: Seq[(String, Boolean)]
  )
  extends DeltaJobStatisticsTracker(
    hadoopConf,
    path,
    dataCols,
    statsColExpr
  ) {

  // Map of column name to its corresponding collected high water mark.
  var highWaterMarks = scala.collection.mutable.Map[String, Long]()

  // Process the stats on the driver. In `stats` we have a sequence of `DeltaFileStatistics`,
  // whose stats is a map of file path to its corresponding array of high water marks in json.
  override def processStats(stats: Seq[WriteTaskStats], jobCommitTime: Long): Unit = {
    stats.map(_.asInstanceOf[DeltaFileStatistics]).flatMap(_.stats).map {
      case (_, statsString) =>
        val fileHighWaterMarks = JsonUtils.fromJson[Array[Long]](statsString)
        // We must have high water marks collected for all IDENTITY columns and we have guaranteed
        // that their orders in the array follow the orders in `identityInfo` by aligning the
        // order of expression and `identityColumnInfo` in `createIdentityColumnStatsTracker`.
        require(fileHighWaterMarks.size == identityColumnInfo.size)
        identityColumnInfo.zip(fileHighWaterMarks).map {
          case ((name, positiveStep), value) =>
            val updated = highWaterMarks.get(name).map { v =>
              if (positiveStep) v.max(value) else v.min(value)
            }.getOrElse(value)
            highWaterMarks.update(name, updated)
        }
    }
  }
}
