#
# Copyright (2021) The Delta Lake Project Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

from typing import overload, Dict, Optional, Union, NoReturn, Any, List, Tuple

from py4j.java_collections import MapConverter as MapConverter  # type: ignore[import]
from py4j.java_gateway import JavaObject  # type: ignore[import]

from pyspark import SparkContext as SparkContext
from pyspark.sql import SQLContext as SQLContext, SparkSession as SparkSession

from pyspark.sql import Column, DataFrame
from pyspark.sql.types import StructType, DataType, StructField

_StringOrColumn = Union[str, Column]

class DeltaTable:
    _spark: SparkSession
    _jdt: JavaObject
    def __init__(self, spark: SparkSession, jdt: JavaObject) -> None: ...
    def toDF(self) -> DataFrame: ...
    def alias(self, aliasName: str) -> DeltaTable: ...
    def generate(self, mode: str) -> None: ...
    def delete(self, condition: Optional[_StringOrColumn] = ...) -> None: ...
    @overload
    def update(
        self, condition: _StringOrColumn, set: Dict[str, _StringOrColumn]
    ) -> None: ...
    @overload
    def update(self, *, set: Dict[str, _StringOrColumn]) -> None: ...
    def merge(
        self, source: DataFrame, condition: _StringOrColumn
    ) -> DeltaMergeBuilder: ...
    def vacuum(self, retentionHours: Optional[float] = ...) -> DataFrame: ...
    def history(self, limit: Optional[int] = ...) -> DataFrame: ...
    @classmethod
    def convertToDelta(
        cls,
        sparkSession: SparkSession,
        identifier: str,
        partitionSchema: Optional[Union[str, StructType]] = ...,
    ) -> DeltaTable: ...
    @classmethod
    def forPath(cls, sparkSession: SparkSession, path: str) -> DeltaTable: ...
    @classmethod
    def forName(
        cls, sparkSession: SparkSession, tableOrViewName: str
    ) -> DeltaTable: ...
    @classmethod
    def create(
        cls, sparkSession: Optional[SparkSession] = ...
    ) -> DeltaTableBuilder: ...
    @classmethod
    def createIfNotExists(
        cls, sparkSession: Optional[SparkSession] = ...
    ) -> DeltaTableBuilder: ...
    @classmethod
    def replace(
        cls, sparkSession: Optional[SparkSession] = ...
    ) -> DeltaTableBuilder: ...
    @classmethod
    def createOrReplace(
        cls, sparkSession: Optional[SparkSession] = ...
    ) -> DeltaTableBuilder: ...
    @classmethod
    def isDeltaTable(cls, sparkSession: SparkSession, identifier: str) -> bool: ...
    def upgradeTableProtocol(self, readerVersion: int, writerVersion: int) -> None: ...
    @classmethod
    def _dict_to_jmap(
        cls,
        sparkSession: SparkSession,
        pydict: Dict[str, _StringOrColumn],
        argname: str,
    ) -> JavaObject: ...
    @classmethod
    def _condition_to_jcolumn(
        cls, condition: Optional[_StringOrColumn], argname: str = ...
    ) -> JavaObject: ...

class DeltaMergeBuilder:
    _spark: SparkSession
    _jbuilder: JavaObject
    def __init__(self, spark: SparkSession, jbuilder: JavaObject) -> None: ...
    @overload
    def whenMatchedUpdate(
        self, condition: Optional[_StringOrColumn], set: Dict[str, _StringOrColumn]
    ) -> DeltaMergeBuilder: ...
    @overload
    def whenMatchedUpdate(
        self, *, set: Dict[str, _StringOrColumn]
    ) -> DeltaMergeBuilder: ...
    def whenMatchedUpdateAll(
        self, condition: Optional[_StringOrColumn] = ...
    ) -> DeltaMergeBuilder: ...
    def whenMatchedDelete(
        self, condition: Optional[_StringOrColumn] = ...
    ) -> DeltaMergeBuilder: ...
    @overload
    def whenNotMatchedInsert(
        self, condition: _StringOrColumn, values: Dict[str, _StringOrColumn]
    ) -> DeltaMergeBuilder: ...
    @overload
    def whenNotMatchedInsert(
        self, *, values: Dict[str, _StringOrColumn] = ...
    ) -> DeltaMergeBuilder: ...
    def whenNotMatchedInsertAll(
        self, condition: Optional[_StringOrColumn] = ...
    ) -> DeltaMergeBuilder: ...
    def execute(self) -> None: ...
    def __getMatchedBuilder(
        self, condition: Optional[_StringOrColumn]
    ) -> JavaObject: ...
    def __getNotMatchedBuilder(
        self, condition: Optional[_StringOrColumn]
    ) -> JavaObject: ...

class DeltaTableBuilder:
    _spark: SparkSession
    _jbuilder: JavaObject
    def __init__(self, spark: SparkSession, jbuilder: JavaObject) -> None: ...
    def tableName(self, identifier: str) -> DeltaTableBuilder: ...
    def location(self, location: str) -> DeltaTableBuilder: ...
    def comment(self, comment: str) -> DeltaTableBuilder: ...
    def addColumn(
        self,
        colName: str,
        dataType: Union[str, DataType],
        nullable: bool = ...,
        generatedAlwaysAs: Optional[str] = ...,
        comment: Optional[str] = ...,
    ) -> DeltaTableBuilder: ...
    def addColumns(
        self, cols: Union[StructType, List[StructField]]
    ) -> DeltaTableBuilder: ...
    def partitionedBy(
        self, *cols: Union[str, List[str], Tuple[str, ...]]
    ) -> DeltaTableBuilder: ...
    def property(self, key: str, value: str) -> DeltaTableBuilder: ...
    def execute(self) -> DeltaTable: ...
    def _raise_type_error(self, msg: str, objs: Any) -> NoReturn: ...
