#!/usr/bin/env python3
"""
Generate Spark version information for CI/CD from CrossSparkVersions.scala

This script reads the JSON file generated by `build/sbt exportSparkVersionsJson`
and provides utilities for GitHub Actions workflows.

The script automatically generates the JSON file if it doesn't exist.

Usage:
    # Get all Spark versions as JSON array
    python project/scripts/get_spark_version_info.py --all-spark-versions
    # Output: ["4.0", "4.1"] or ["master", "4.0"] if master is present

    # Get only released Spark versions (no snapshots)
    python project/scripts/get_spark_version_info.py --released-spark-versions
    # Output: ["4.0", "4.1"] (excludes versions with -SNAPSHOT)

    # Get a specific field for a Spark version (using short version or "master")
    python project/scripts/get_spark_version_info.py --get-field 4.0 targetJvm
    python project/scripts/get_spark_version_info.py --get-field master targetJvm
    # Output: "17"
"""

import argparse
import json
import subprocess
import sys
from pathlib import Path


def generate_spark_versions_json(repo_root: Path) -> bool:
    """Generate the spark-versions.json file by running sbt exportSparkVersionsJson."""
    try:
        print("Generating spark-versions.json...", file=sys.stderr)
        subprocess.run(
            ["build/sbt", "exportSparkVersionsJson"],
            cwd=repo_root,
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.PIPE
        )
        return True
    except subprocess.CalledProcessError as e:
        print(f"ERROR: Failed to generate spark-versions.json: {e}", file=sys.stderr)
        return False


def load_spark_versions(json_path: Path, repo_root: Path):
    """Load Spark versions from JSON file, generating it if necessary."""
    if not json_path.exists():
        if not generate_spark_versions_json(repo_root):
            sys.exit(1)
    
    if not json_path.exists():
        print(
            f"ERROR: spark-versions.json not found at {json_path} even after generation",
            file=sys.stderr
        )
        sys.exit(1)

    with open(json_path, 'r') as f:
        return json.load(f)


def main():
    parser = argparse.ArgumentParser(
        description="Generate Spark version information from CrossSparkVersions.scala"
    )
    parser.add_argument(
        "--all-spark-versions",
        action="store_true",
        help="Output all Spark versions as JSON array (e.g., [\"4.0\", \"4.1\"] or [\"master\", \"4.0\"])"
    )
    parser.add_argument(
        "--released-spark-versions",
        action="store_true",
        help="Output only released Spark versions (excluding snapshots) as JSON array"
    )
    parser.add_argument(
        "--get-field",
        nargs=2,
        metavar=("SPARK_VERSION", "FIELD"),
        help="Get a specific field for a Spark version (e.g., --get-field 4.0 targetJvm or --get-field master targetJvm)"
    )

    args = parser.parse_args()

    # Determine JSON path (relative to repo root)
    script_dir = Path(__file__).parent
    repo_root = script_dir.parent.parent
    json_path = repo_root / "target" / "spark-versions.json"

    try:
        versions = load_spark_versions(json_path, repo_root)

        if args.all_spark_versions:
            # For master version, use "master"; for others, use short version
            matrix_versions = []
            for v in versions:
                if v.get("isMaster", False):
                    matrix_versions.append("master")
                else:
                    matrix_versions.append(v["shortVersion"])
            print(json.dumps(matrix_versions))

        elif args.released_spark_versions:
            # Only include released versions (no -SNAPSHOT in fullVersion)
            matrix_versions = []
            for v in versions:
                if "-SNAPSHOT" not in v["fullVersion"]:
                    matrix_versions.append(v["shortVersion"])
            print(json.dumps(matrix_versions))

        elif args.get_field:
            spark_version, field = args.get_field
            
            # Find the version entry by matching:
            # - "master" matches isMaster=true
            # - short version like "4.0" matches shortVersion
            # - full version like "4.0.1" matches fullVersion
            version_entry = None
            for v in versions:
                if spark_version == "master" and v.get("isMaster", False):
                    version_entry = v
                    break
                elif spark_version == v["shortVersion"] or spark_version == v["fullVersion"]:
                    version_entry = v
                    break

            if not version_entry:
                print(f"ERROR: Spark version '{spark_version}' not found", file=sys.stderr)
                sys.exit(1)

            if field not in version_entry:
                print(
                    f"ERROR: Field '{field}' not found for Spark version {spark_version}\n"
                    f"Available fields: {', '.join(version_entry.keys())}",
                    file=sys.stderr
                )
                sys.exit(1)

            # Print as JSON for proper formatting
            print(json.dumps(version_entry[field]))

        else:
            parser.print_help()
            sys.exit(1)

    except Exception as e:
        print(f"ERROR: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
