#!/usr/bin/env python3
"""
Generate Spark version information for CI/CD from CrossSparkVersions.scala

This script reads the JSON file generated by `build/sbt exportSparkVersionsJson`
and provides utilities to:
1. Generate GitHub Actions matrix values
2. Provide version information for Python tests
3. Extract version-specific details (JVM version, etc.)
4. Validate consistency across the build system

Usage:
    # Generate GitHub Actions matrix JSON
    python project/scripts/generate_spark_versions.py --github-matrix

    # Get all Spark versions (one per line)
    python project/scripts/generate_spark_versions.py --list

    # Get default Spark version
    python project/scripts/generate_spark_versions.py --default

    # Get version info as Python dict
    python project/scripts/generate_spark_versions.py --python-dict

    # Get full JSON with all version details
    python project/scripts/generate_spark_versions.py --full-json

    # Get a specific field for a Spark version
    python project/scripts/generate_spark_versions.py --get-field 4.0.1 targetJvm
    python project/scripts/generate_spark_versions.py --get-field 4.0.1 shortVersion

    # Validate that JSON file exists and is up-to-date
    python project/scripts/generate_spark_versions.py --validate
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List, Any


class SparkVersionInfo:
    """Wrapper for Spark version information from CrossSparkVersions.scala"""

    def __init__(self, json_path: Path):
        """Load Spark version information from JSON file."""
        if not json_path.exists():
            raise FileNotFoundError(
                f"Spark versions JSON not found at {json_path}.\n"
                f"Please run: build/sbt exportSparkVersionsJson"
            )

        with open(json_path, 'r') as f:
            self.data = json.load(f)

    @property
    def default_version(self) -> str:
        """Get the default Spark version (full version string)."""
        return self.data["default"]

    @property
    def default_short_version(self) -> str:
        """Get the default Spark short version (e.g., '4.0')."""
        return self.data["defaultShortVersion"]

    @property
    def all_versions(self) -> List[Dict[str, Any]]:
        """Get all Spark version specifications."""
        return self.data["allVersions"]

    @property
    def all_full_versions(self) -> List[str]:
        """Get list of all full version strings."""
        return [v["fullVersion"] for v in self.all_versions]

    @property
    def all_short_versions(self) -> List[str]:
        """Get list of all short version strings."""
        return [v["shortVersion"] for v in self.all_versions]

    def get_suffix(self, full_version: str) -> str:
        """Get the artifact suffix for a given Spark version."""
        for v in self.all_versions:
            if v["fullVersion"] == full_version:
                return v["suffix"]
        raise ValueError(f"Unknown Spark version: {full_version}")

    def get_short_version(self, full_version: str) -> str:
        """Get the short version for a given full version."""
        for v in self.all_versions:
            if v["fullVersion"] == full_version:
                return v["shortVersion"]
        raise ValueError(f"Unknown Spark version: {full_version}")

    def is_default(self, full_version: str) -> bool:
        """Check if a version is the default version."""
        for v in self.all_versions:
            if v["fullVersion"] == full_version:
                return v["isDefault"]
        raise ValueError(f"Unknown Spark version: {full_version}")

    def get_version_details(self, full_version: str) -> Dict[str, Any]:
        """Get all details for a given Spark version."""
        for v in self.all_versions:
            if v["fullVersion"] == full_version:
                return v
        raise ValueError(f"Unknown Spark version: {full_version}")

    def get_target_jvm(self, full_version: str) -> str:
        """Get the target JVM version for a given Spark version."""
        return self.get_version_details(full_version)["targetJvm"]

    def github_matrix_json(self) -> str:
        """Generate GitHub Actions matrix JSON for spark_version."""
        versions = self.all_full_versions
        return json.dumps(versions)

    def python_dict_str(self) -> str:
        """Generate Python dict string for use in Python tests."""
        lines = ["{"]
        for v in self.all_versions:
            suffix = v["suffix"]
            lines.append(f'    "{v["fullVersion"]}": SparkVersionSpec("{suffix}"),')
        lines.append("}")
        return "\n".join(lines)

    def validate(self) -> bool:
        """Validate the JSON structure."""
        required_keys = ["default", "defaultShortVersion", "master", "masterShortVersion", "allVersions"]
        for key in required_keys:
            if key not in self.data:
                print(f"ERROR: Missing required key '{key}' in JSON", file=sys.stderr)
                return False

        if not self.all_versions:
            print("ERROR: No Spark versions found in JSON", file=sys.stderr)
            return False

        # Validate that default version is in all versions
        if self.default_version not in self.all_full_versions:
            print(f"ERROR: Default version '{self.default_version}' not found in all versions", file=sys.stderr)
            return False

        return True


def main():
    parser = argparse.ArgumentParser(
        description="Generate Spark version information from CrossSparkVersions.scala"
    )
    parser.add_argument(
        "--github-matrix",
        action="store_true",
        help="Output GitHub Actions matrix JSON for spark_version"
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="List all Spark versions (one per line)"
    )
    parser.add_argument(
        "--default",
        action="store_true",
        help="Output the default Spark version"
    )
    parser.add_argument(
        "--python-dict",
        action="store_true",
        help="Output Python dict for use in tests"
    )
    parser.add_argument(
        "--validate",
        action="store_true",
        help="Validate the JSON file"
    )
    parser.add_argument(
        "--full-json",
        action="store_true",
        help="Output the complete JSON with all details"
    )
    parser.add_argument(
        "--get-field",
        nargs=2,
        metavar=("SPARK_VERSION", "FIELD"),
        help="Get a specific field for a Spark version (e.g., --get-field 4.0.1 targetJvm)"
    )
    parser.add_argument(
        "--json-path",
        type=Path,
        default=None,
        help="Path to spark-versions.json (default: target/spark-versions.json)"
    )

    args = parser.parse_args()

    # Determine JSON path
    if args.json_path:
        json_path = args.json_path
    else:
        # Default to target/spark-versions.json relative to repo root
        script_dir = Path(__file__).parent
        repo_root = script_dir.parent.parent
        json_path = repo_root / "target" / "spark-versions.json"

    try:
        info = SparkVersionInfo(json_path)

        if args.validate:
            if info.validate():
                print("âœ“ Spark versions JSON is valid")
                sys.exit(0)
            else:
                sys.exit(1)

        elif args.github_matrix:
            print(info.github_matrix_json())

        elif args.list:
            for version in info.all_full_versions:
                print(version)

        elif args.default:
            print(info.default_version)

        elif args.python_dict:
            print(info.python_dict_str())

        elif args.full_json:
            print(json.dumps(info.data, indent=2))

        elif args.get_field:
            spark_version, field = args.get_field
            details = info.get_version_details(spark_version)
            if field not in details:
                print(f"ERROR: Field '{field}' not found for Spark version {spark_version}", file=sys.stderr)
                print(f"Available fields: {', '.join(details.keys())}", file=sys.stderr)
                sys.exit(1)
            value = details[field]
            # Print as JSON for proper formatting
            print(json.dumps(value))

        else:
            # Default: print all information
            print(f"Default Spark version: {info.default_version}")
            print(f"All Spark versions: {', '.join(info.all_full_versions)}")
            print(f"\nGitHub Actions matrix JSON:")
            print(info.github_matrix_json())

    except FileNotFoundError as e:
        print(f"ERROR: {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"ERROR: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

