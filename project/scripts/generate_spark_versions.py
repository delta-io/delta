#!/usr/bin/env python3
"""
Generate Spark version information for CI/CD from CrossSparkVersions.scala

This script reads the JSON file generated by `build/sbt exportSparkVersionsJson`
and provides utilities for GitHub Actions workflows.

Usage:
    # Generate GitHub Actions matrix JSON
    python project/scripts/generate_spark_versions.py --github-matrix
    # Output: ["4.0.1", "4.1.0"]

    # Get a specific field for a Spark version
    python project/scripts/generate_spark_versions.py --get-field 4.0.1 targetJvm
    # Output: "17"
"""

import argparse
import json
import sys
from pathlib import Path


def load_spark_versions(json_path: Path):
    """Load Spark versions from JSON file."""
    if not json_path.exists():
        print(
            f"ERROR: Spark versions JSON not found at {json_path}\n"
            f"Please run: build/sbt exportSparkVersionsJson",
            file=sys.stderr
        )
        sys.exit(1)

    with open(json_path, 'r') as f:
        return json.load(f)


def main():
    parser = argparse.ArgumentParser(
        description="Generate Spark version information from CrossSparkVersions.scala"
    )
    parser.add_argument(
        "--github-matrix",
        action="store_true",
        help="Output GitHub Actions matrix JSON for spark_version"
    )
    parser.add_argument(
        "--get-field",
        nargs=2,
        metavar=("SPARK_VERSION", "FIELD"),
        help="Get a specific field for a Spark version (e.g., --get-field 4.0.1 targetJvm)"
    )

    args = parser.parse_args()

    # Determine JSON path (relative to repo root)
    script_dir = Path(__file__).parent
    repo_root = script_dir.parent.parent
    json_path = repo_root / "target" / "spark-versions.json"

    try:
        versions = load_spark_versions(json_path)

        if args.github_matrix:
            # Extract full versions for matrix
            full_versions = [v["fullVersion"] for v in versions]
            print(json.dumps(full_versions))

        elif args.get_field:
            spark_version, field = args.get_field
            # Find the version entry
            version_entry = None
            for v in versions:
                if v["fullVersion"] == spark_version:
                    version_entry = v
                    break

            if not version_entry:
                print(f"ERROR: Spark version '{spark_version}' not found", file=sys.stderr)
                sys.exit(1)

            if field not in version_entry:
                print(
                    f"ERROR: Field '{field}' not found for Spark version {spark_version}\n"
                    f"Available fields: {', '.join(version_entry.keys())}",
                    file=sys.stderr
                )
                sys.exit(1)

            # Print as JSON for proper formatting
            print(json.dumps(version_entry[field]))

        else:
            parser.print_help()
            sys.exit(1)

    except Exception as e:
        print(f"ERROR: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
