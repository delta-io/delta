name: "Delta Spark Publishing and Examples"
on:
  push:
    paths-ignore:
      - '**.md'
      - '**.txt'
  pull_request:
    paths-ignore:
      - '**.md'
      - '**.txt'
jobs:
  # Generate Spark versions matrix from CrossSparkVersions.scala
  # This workflow tests against released versions only (no snapshots)
  generate-matrix:
    name: "Generate Released Spark Versions Matrix"
    runs-on: ubuntu-24.04
    outputs:
      spark_versions: ${{ steps.generate.outputs.spark_versions }}
    steps:
      - uses: actions/checkout@v3
      - name: install java
        uses: actions/setup-java@v3
        with:
          distribution: "zulu"
          java-version: "17"
      - name: Generate released Spark versions matrix
        id: generate
        run: |
          # Get only released versions (exclude snapshots)
          SPARK_VERSIONS=$(python3 project/scripts/get_spark_version_info.py --released-spark-versions)
          echo "spark_versions=$SPARK_VERSIONS" >> $GITHUB_OUTPUT
          echo "Generated released Spark versions: $SPARK_VERSIONS"

  test:
    name: "DSP&E: Spark ${{ matrix.spark_version }}, Scala ${{ matrix.scala }}"
    runs-on: ubuntu-24.04
    needs: generate-matrix
    strategy:
      matrix:
        # Spark versions are dynamically generated - released versions only
        spark_version: ${{ fromJson(needs.generate-matrix.outputs.spark_versions) }}
        # These Scala versions must match those in the build.sbt
        scala: [2.13.17]
    env:
      SCALA_VERSION: ${{ matrix.scala }}
    steps:
      - uses: actions/checkout@v3
      - name: Get Spark version details
        id: spark-details
        run: |
          # Get JVM version, package suffix, iceberg support, and full version for this Spark version
          JVM_VERSION=$(python3 project/scripts/get_spark_version_info.py --get-field "${{ matrix.spark_version }}" targetJvm | jq -r)
          SPARK_PACKAGE_SUFFIX=$(python3 project/scripts/get_spark_version_info.py --get-field "${{ matrix.spark_version }}" packageSuffix | jq -r)
          SUPPORT_ICEBERG=$(python3 project/scripts/get_spark_version_info.py --get-field "${{ matrix.spark_version }}" supportIceberg | jq -r)
          SPARK_FULL_VERSION=$(python3 project/scripts/get_spark_version_info.py --get-field "${{ matrix.spark_version }}" fullVersion | jq -r)
          echo "jvm_version=$JVM_VERSION" >> $GITHUB_OUTPUT
          echo "spark_package_suffix=$SPARK_PACKAGE_SUFFIX" >> $GITHUB_OUTPUT
          echo "support_iceberg=$SUPPORT_ICEBERG" >> $GITHUB_OUTPUT
          echo "spark_full_version=$SPARK_FULL_VERSION" >> $GITHUB_OUTPUT
          echo "Using JVM $JVM_VERSION for Spark $SPARK_FULL_VERSION, package suffix: '$SPARK_PACKAGE_SUFFIX', support iceberg: '$SUPPORT_ICEBERG'"
      - name: install java
        uses: actions/setup-java@v3
        with:
          distribution: "zulu"
          java-version: ${{ steps.spark-details.outputs.jvm_version }}
      - name: Cache Scala, SBT
        uses: actions/cache@v3
        with:
          path: |
            ~/.sbt
            ~/.ivy2
            ~/.cache/coursier
          # Change the key if dependencies are changed. For each key, GitHub Actions will cache the
          # the above directories when we use the key for the first time. After that, each run will
          # just use the cache. The cache is immutable so we need to use a new key when trying to
          # cache new stuff.
          key: delta-sbt-cache-spark${{ matrix.spark_version }}-scala${{ matrix.scala }}
      - name: Install Job dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python3-openssl git
          sudo apt install libedit-dev
      - name: Run Delta Spark Local Publishing and Examples Compilation
        # examples/scala/build.sbt will compile against the local Delta release version (e.g. 3.2.0-SNAPSHOT).
        # Thus, we need to publishM2 first so those jars are locally accessible.
        # -DsparkVersion is for the Delta project's publishM2 (which Spark version to compile Delta against).
        # SPARK_VERSION/SPARK_PACKAGE_SUFFIX/SUPPORT_ICEBERG are for examples/scala/build.sbt (dependency resolution).
        env:
          SPARK_PACKAGE_SUFFIX: ${{ steps.spark-details.outputs.spark_package_suffix }}
          SUPPORT_ICEBERG: ${{ steps.spark-details.outputs.support_iceberg }}
          SPARK_VERSION: ${{ steps.spark-details.outputs.spark_full_version }}
        run: |
          build/sbt clean
          build/sbt -DsparkVersion=${{ steps.spark-details.outputs.spark_full_version }} publishM2
          cd examples/scala && build/sbt "++ $SCALA_VERSION compile"
