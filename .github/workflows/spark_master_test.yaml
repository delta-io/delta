name: "Delta Spark Master Tests"
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-20.04
    strategy:
      matrix:
        # These Scala versions must match those in the build.sbt
        scala: [2.13.13]
    env:
      SCALA_VERSION: ${{ matrix.scala }}
    steps:
      - uses: actions/checkout@v3
      - uses: technote-space/get-diff-action@v4
        id: git-diff
        with:
          PATTERNS: |
            **
            .github/workflows/**
            !kernel/**
            !connectors/**
      - name: install java
        uses: actions/setup-java@v3
        with:
          distribution: "zulu"
          java-version: "17"
      - name: Cache Scala, SBT
        uses: actions/cache@v3
        with:
          path: |
            ~/.sbt
            ~/.ivy2
            ~/.cache/coursier
            !~/.cache/coursier/v1/https/repository.apache.org/content/groups/snapshots
          # Change the key if dependencies are changed. For each key, GitHub Actions will cache the
          # the above directories when we use the key for the first time. After that, each run will
          # just use the cache. The cache is immutable so we need to use a new key when trying to
          # cache new stuff.
          key: delta-sbt-cache-spark-master-scala${{ matrix.scala }}
      - name: Install Job dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl git
          sudo apt install libedit-dev
          curl -LO https://github.com/bufbuild/buf/releases/download/v1.28.1/buf-Linux-x86_64.tar.gz
          mkdir -p ~/buf
          tar -xvzf buf-Linux-x86_64.tar.gz -C ~/buf --strip-components 1
          rm buf-Linux-x86_64.tar.gz
          sudo apt install python3-pip --fix-missing
          sudo pip3 install pipenv==2021.5.29
          curl https://pyenv.run | bash
          export PATH="~/.pyenv/bin:$PATH"
          eval "$(pyenv init -)"
          eval "$(pyenv virtualenv-init -)"
          pyenv install 3.9
          pyenv global system 3.9
          pipenv --python 3.9 install
          pipenv run pip install flake8==3.9.0
          pipenv run pip install black==23.9.1
          pipenv run pip install importlib_metadata==3.10.0
          pipenv run pip install mypy==1.8.0
          pipenv run pip install mypy-protobuf==3.3.0
          pipenv run pip install cryptography==37.0.4
          pipenv run pip install twine==4.0.1
          pipenv run pip install wheel==0.33.4
          pipenv run pip install setuptools==41.1.0
          pipenv run pip install pydocstyle==3.0.0
          pipenv run pip install pandas==2.0.0
          pipenv run pip install pyarrow==10.0.0
          pipenv run pip install pypandoc==1.3.3
          pipenv run pip install numpy==1.21
          pipenv run pip install grpcio==1.62.0
          pipenv run pip install grpcio-status==1.62.0
          pipenv run pip install googleapis-common-protos==1.56.4
          pipenv run pip install protobuf==4.25.1
          pipenv run pip install googleapis-common-protos-stubs==2.2.0
          pipenv run pip install grpc-stubs==1.24.11
          pipenv run pip install pyspark==4.0.0.dev1
        if: steps.git-diff.outputs.diff
      - name: Run Spark Master tests
        # when changing TEST_PARALLELISM_COUNT make sure to also change it in spark_test.yaml
        run: |
          TEST_PARALLELISM_COUNT=2 build/sbt -DsparkVersion=master "++ ${{ matrix.scala }}" clean spark/test
          TEST_PARALLELISM_COUNT=2 build/sbt -DsparkVersion=master "++ ${{ matrix.scala }}" clean connectServer/test
          TEST_PARALLELISM_COUNT=2 build/sbt -DsparkVersion=master "++ ${{ matrix.scala }}" clean connectServer/assembly connectClient/test
          TEST_PARALLELISM_COUNT=2 PYTHON_TESTS_ONLY=true pipenv run python run-tests.py
        if: steps.git-diff.outputs.diff
